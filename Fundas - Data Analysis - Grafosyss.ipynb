{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes by - Kiran A Bendigeri\n",
    "Please Read 'Read me' file.\n",
    "\n",
    "1 Data Analysis with Python and Pandas Tutorial Introduction\n",
    " The Pandas module is a high performance, highly efficient, and high level data analysis library.At its core, it is very much like operating a headless version of a spreadsheet, like Excel. Most of the datasets you work with will be what are called dataframes. You may be familiar with this term already, it is used across other languages, but, if not, a dataframe is most often just like a spreadsheet. Columns and rows, that's all there is to it! From here, we can utilize Pandas to perform operations on our data sets at lightning speeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-069f571ec148>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2010\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\win7\\python36\\lib\\site-packages\\pandas\\io\\data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m raise ImportError(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;34m\"The pandas.io.data module is moved to a separate package \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"(pandas-datareader). After installing the pandas-datareader package \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"(https://github.com/pydata/pandas-datareader), you can change \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"the import ``from pandas.io import data, wb`` to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: The pandas.io.data module is moved to a separate package (pandas-datareader). After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas.io.data as web\n",
    "\n",
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = datetime.datetime(2015, 8, 22)\n",
    "\n",
    "df = web.DataReader(\"XOM\", \"yahoo\", start, end)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "df['High'].plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Pandas Basics - p.2 Data Analysis with Python and Pandas Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "web_stats = {'Day':[1,2,3,4,5,6],\n",
    "             'Visitors':[43,34,65,56,29,76],\n",
    "             'Bounce Rate':[65,67,78,65,45,52]}\n",
    "\n",
    "df = pd.DataFrame(web_stats)\n",
    "\n",
    "df.set_index('Day', inplace=True)\n",
    "\n",
    "print(df.Visitors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 IO Basics - p.3 Data Analysis with Python and Pandas Tutorial\n",
    " we will begin discussing IO, or input/output, with Pandas, and begin with a realistic use-case. To get ample practice, a very useful website is Quandl. Quandl contains a plethora of free and paid data sources. What makes this location great is that the data is generally normalized, it's all in one place, and extracting the data is the same method. If you are using Python, and you access the Quandl data via their simple module, then the data is automatically returned to a dataframe. For the purposes of this tutorial, we're going to just manually download a CSV file instead, for learning purposes, since not every data source you find is going to have a nice and neat module for extracting the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('ZILL-Z77006_3B.csv')\n",
    "print(df.head())\n",
    "\n",
    "df.set_index('Date', inplace = True)\n",
    "\n",
    "df.to_csv('newcsv2.csv')\n",
    "\n",
    "df['Value'].to_csv('newcsv2.csv')\n",
    "\n",
    "df.columns = ['House_Prices']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Building dataset - p.4 Data Analysis with Python and Pandas Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Quandl\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "df = Quandl.get(\"FMAC/HPI_TX\", authtoken=api_key)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "fiddy_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states')\n",
    "print(fiddy_states)\n",
    "\n",
    "print(fiddy_states[0])\n",
    "\n",
    "for abbv in fiddy_states[0][0][1:]:\n",
    "    print(abbv)\n",
    "    \n",
    "for abbv in fiddy_states[0][0][1:]:\n",
    "    #print(abbv)\n",
    "    print(\"FMAC/HPI_\"+str(abbv))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Concatenating and Appending dataframes - p.5 Data Analysis with Python and Pandas Tutorial\n",
    "There are four major ways of combining dataframes, which we'll begin covering now. The four major ways are: Concatenation, joining, merging, and appending. We'll begin with Concatenation.\n",
    "Appending is like the first example of concatenation, only a bit more forceful in that the dataframe will simply be appended to, adding to rows.\n",
    "A series is basically a single-columned dataframe. A series does have an index, but, if you convert it to a list, it will be just those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      HPI  Int_rate  US_GDP_Thousands\n",
      "2001   80         2                50\n",
      "2002   85         3                55\n",
      "2003   88         2                65\n",
      "2004   85         2                55\n",
      "2005   80         2                50\n",
      "2006   85         3                55\n",
      "2007   88         2                65\n",
      "2008   85         2                55\n",
      "      HPI  Int_rate  US_GDP_Thousands\n",
      "2001   80         2                50\n",
      "2002   85         3                55\n",
      "2003   88         2                65\n",
      "2004   85         2                55\n",
      "2005   80         2                50\n",
      "2006   85         3                55\n",
      "2007   88         2                65\n",
      "2008   85         2                55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'HPI':[80,85,88,85],\n",
    "                    'Int_rate':[2, 3, 2, 2],\n",
    "                    'US_GDP_Thousands':[50, 55, 65, 55]},\n",
    "                   index = [2001, 2002, 2003, 2004])\n",
    "\n",
    "df2 = pd.DataFrame({'HPI':[80,85,88,85],\n",
    "                    'Int_rate':[2, 3, 2, 2],\n",
    "                    'US_GDP_Thousands':[50, 55, 65, 55]},\n",
    "                   index = [2005, 2006, 2007, 2008])\n",
    "\n",
    "df3 = pd.DataFrame({'HPI':[80,85,88,85],\n",
    "                    'Int_rate':[2, 3, 2, 2],\n",
    "                    'Low_tier_HPI':[50, 52, 50, 53]},\n",
    "                   index = [2001, 2002, 2003, 2004])\n",
    "\n",
    "concat = pd.concat([df1,df2])\n",
    "print(concat)\n",
    "\n",
    "df4 = df1.append(df2)\n",
    "print(df4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Joining and Merging Dataframes - p.6 Data Analysis with Python and Pandas Tutorial\n",
    " After mergin, you would probably set the new index. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.merge(df1,df3, on='HPI')\n",
    "df4.set_index('HPI', inplace=True)\n",
    "print(df4)\n",
    "\n",
    "df1.set_index('HPI', inplace=True)\n",
    "df3.set_index('HPI', inplace=True)\n",
    "\n",
    "joined = df1.join(df3)\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Pickling - p.7 Data Analysis with Python and Pandas Tutorial\n",
    "You generally train a classifier, and then you can start immediately, and quickly, classifying with that classifier. The problem is, a classifer can't be saved to a .txt or .csv file. It's an object. Luckily, in programming, there are various terms for the process of saving binary data to a file that can be accessed later. In Python, this is called pickling. You may know it as serialization, or maybe even something else. Python has a module called Pickle, which will convert your object to a byte stream, or the reverse with unpickling. What this lets us do is save any Python object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Quandl\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "def state_list():\n",
    "    fiddy_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states')\n",
    "    return fiddy_states[0][0][1:]\n",
    "    \n",
    "\n",
    "def grab_initial_state_data():\n",
    "    states = state_list()\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for abbv in states:\n",
    "        query = \"FMAC/HPI_\"+str(abbv)\n",
    "        df = Quandl.get(query, authtoken=api_key)\n",
    "        print(query)\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "            \n",
    "    pickle_out = open('fiddy_states.pickle','wb')\n",
    "    pickle.dump(main_df, pickle_out)\n",
    "    pickle_out.close()        \n",
    "\n",
    "    \n",
    "grab_initial_state_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Percent Change and Correlation Tables - p.8 Data Analysis with Python and Pandas Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_initial_state_data():\n",
    "    states = state_list()\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for abbv in states:\n",
    "        query = \"FMAC/HPI_\"+str(abbv)\n",
    "        df = Quandl.get(query, authtoken=api_key)\n",
    "        print(query)\n",
    "        df[abbv] = (df[abbv]-df[abbv][0]) / df[abbv][0] * 100.0\n",
    "        print(df.head())\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "            \n",
    "    pickle_out = open('fiddy_states3.pickle','wb')\n",
    "    pickle.dump(main_df, pickle_out)\n",
    "    pickle_out.close()\n",
    "\t\n",
    "grab_initial_state_data()   \n",
    "\n",
    "HPI_data = pd.read_pickle('fiddy_states3.pickle')\n",
    "\n",
    "HPI_data.plot()\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 Resampling - p.9 Data Analysis with Python and Pandas Tutorial\n",
    "smoothing out data by removing noise. There are two main methods to do this. The most popular method used is what is called resampling, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Quandl\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "def state_list():\n",
    "    fiddy_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states')\n",
    "    return fiddy_states[0][0][1:]\n",
    "    \n",
    "\n",
    "def grab_initial_state_data():\n",
    "    states = state_list()\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for abbv in states:\n",
    "        query = \"FMAC/HPI_\"+str(abbv)\n",
    "        df = Quandl.get(query, authtoken=api_key)\n",
    "        print(query)\n",
    "        df[abbv] = (df[abbv]-df[abbv][0]) / df[abbv][0] * 100.0\n",
    "        print(df.head())\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "            \n",
    "    pickle_out = open('fiddy_states3.pickle','wb')\n",
    "    pickle.dump(main_df, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "def HPI_Benchmark():\n",
    "    df = Quandl.get(\"FMAC/HPI_USA\", authtoken=api_key)\n",
    "    df[\"United States\"] = (df[\"United States\"]-df[\"United States\"][0]) / df[\"United States\"][0] * 100.0\n",
    "    return df\n",
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1,1), (0,0))\n",
    "HPI_data = pd.read_pickle('fiddy_states3.pickle')\n",
    "HPI_State_Correlation = HPI_data.corr()\n",
    "\n",
    "TX1yr = HPI_data['TX'].resample('A')\n",
    "print(TX1yr.head())\n",
    "\n",
    "HPI_data['TX'].plot(ax=ax1)\n",
    "TX1yr.plot(color='k',ax=ax1)\n",
    "\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Handling Missing Data - p.10 Data Analysis with Python and Pandas Tutorial\n",
    "We have a few options when considering the existence of missing data.\n",
    "\n",
    "Ignore it - Just leave it there\n",
    "Delete it - Remove all cases. Remove from data entirely. This means forfeiting the entire row of data.\n",
    "Fill forward or backwards - This means taking the prior or following value and just filling it in.\n",
    "Replace it with something static - For example, replacing all NaN data with -9999.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPI_data.dropna(inplace=True)\n",
    "print(HPI_data[['TX','TX1yr']])\n",
    "\n",
    "HPI_data.dropna(how='all',inplace=True)\n",
    "\n",
    "HPI_data.fillna(method='ffill',inplace=True)\n",
    "\n",
    "HPI_data.fillna(method='bfill',inplace=True)\n",
    "\n",
    "HPI_data.fillna(value=-99999,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Rolling statistics - p.11 Data Analysis with Python and Pandas Tutorial\n",
    "One of the more popular rolling statistics is the moving average. This takes a moving window of time, and calculates the average or the mean of that time period as the current value. In our case, we have monthly data. So a 10 moving average would be the current value, plus the previous 9 months of data, averaged, and there we would have a 10 moving average of our monthly data. Doing this is Pandas is incredibly fast. Pandas comes with a few pre-made rolling statistical functions, but also has one called a rolling_apply. This allows us to write our own function that accepts window data and apply any bit of logic we want that is reasonable. This means that even if Pandas doesn't officially have a function to handle what you want, they have you covered and allow you to write exactly what you need. Let's start with a basic moving average, or a rolling_mean as Pandas calls it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPI_data['TX12MA'] = pd.rolling_mean(HPI_data['TX'], 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 Applying Comparison Operators to DataFrame - p.12 Data Analysis with Python and Pandas Tutorial\n",
    "we're goign to talk briefly on the handling of erroneous/outlier data. Just because data is an outlier, it does not mean it is erroneous. A lot of times, an outlier data point can nullify a hypothesis, so the urge to just get rid of it can be high, but this isn't what we're talking about here.\n",
    "\n",
    "What would an erroneous outlier be? An example I like to use is when measuring fluctuations in something like, say, a bridge. As bridges carry weight, they can move a bit. In storms, that can wiggle about a bit, there is some natural movement. As time goes on, and supports weaken, the bridge might move a bit too much, and eventually need to be reinforced. Maybe we have a system in place that constantly measures fluctuations in the bridge's height.\n",
    "\n",
    "Preprocessing is used to adjust our dataset. Typically, machine learning will be a bit more accurate if your features are between -1 and 1 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_height = {'meters':[10.26, 10.31, 10.27, 10.22, 10.23, 6212.42, 10.28, 10.25, 10.31]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 Scikit Learn Incorporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Quandl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "def create_labels(cur_hpi, fut_hpi):\n",
    "    if fut_hpi > cur_hpi:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def moving_average(values):\n",
    "    return mean(values)\n",
    "\n",
    "housing_data = pd.read_pickle('HPI.pickle')\n",
    "housing_data = housing_data.pct_change()\n",
    "housing_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "housing_data['US_HPI_future'] = housing_data['United States'].shift(-1)\n",
    "housing_data.dropna(inplace=True)\n",
    "#print(housing_data[['US_HPI_future','United States']].head())\n",
    "housing_data['label'] = list(map(create_labels,housing_data['United States'], housing_data['US_HPI_future']))\n",
    "#print(housing_data.head())\n",
    "housing_data['ma_apply_example'] = pd.rolling_apply(housing_data['M30'], 10, moving_average)\n",
    "print(housing_data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, preprocessing, cross_validation\n",
    "\n",
    "X = np.array(housing_data.drop(['label','US_HPI_future'], 1))\n",
    "X = preprocessing.scale(X)\n",
    "y = np.array(housing_data['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
